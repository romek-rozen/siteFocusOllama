import os
os.environ["LOKY_MAX_CPU_COUNT"] = str(os.cpu_count())
import streamlit as st
from streamlit.components.v1 import html
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.manifold import TSNE
from numpy.linalg import norm
import requests
from bs4 import BeautifulSoup
import re
import matplotlib.pyplot as plt
import io
from sklearn.metrics.pairwise import cosine_similarity
from urllib.parse import urlparse
from openai import OpenAI
import tiktoken
from markdownify import MarkdownConverter as md
import uuid
import atexit
from concurrent.futures import ThreadPoolExecutor

st.set_page_config(
    page_title="SiteFocus Tool - Analiza sp√≥jno≈õci tematycznej",
    page_icon="üéØ",
    initial_sidebar_state="expanded",
)

# Dla r√≥wnoleg≈Çego przetwarzania
executor = ThreadPoolExecutor(max_workers=5)  # Limit r√≥wnoleg≈Çych request√≥w

async def process_result(result):
    """Przetwarza pojedynczy wynik crawlowania."""
    if result.success:
        print(f"Sukces dla {result.url}")
        return True
    else:
        print(f"Niepowodzenie dla {result.url}: {result.error_message}")
        return False

# Na poczƒÖtku pliku, gdzie inicjalizujemy inne session_state
if 'crawl_cache' not in st.session_state:
    st.session_state.crawl_cache = {}

if 'url_status_cache' not in st.session_state:
    st.session_state.url_status_cache = {}

if 'model_list' not in st.session_state:
    st.session_state.model_list = []
if 'selected_model' not in st.session_state:
    st.session_state.selected_model = None
if 'model_cache' not in st.session_state:  # Dodajemy cache dla wybranego modelu
    st.session_state.model_cache = None
if 'model_initialized' not in st.session_state:  # Nowa flaga
    st.session_state.model_initialized = False
if 'analysis_results' not in st.session_state:  # Dodajemy cache dla wynik√≥w
    st.session_state.analysis_results = {}

# Dodajmy przycisk do czyszczenia cache crawla obok przycisku czyszczenia cache embedding√≥w
if st.sidebar.button("Wyczy≈õƒá cache crawla"):
    st.session_state.crawl_cache = {}
    st.success("Cache crawla zosta≈Ç wyczyszczony!")

# Na poczƒÖtku pliku, gdzie inicjalizujemy session_state
if 'api_keys' not in st.session_state:
    st.session_state.api_keys = {
        'openai': None,
        'jina': None
    }

def clean_text(text):
    """Czy≈õci i formatuje tekst."""
    # Usuwamy nadmiarowe bia≈Çe znaki
    text = ' '.join(text.split())
    # Usuwamy znaki specjalne (opr√≥cz kropek i przecink√≥w)
    text = re.sub(r'[^\w\s.,]', ' ', text)
    # Zamieniamy wielokrotne spacje na pojedyncze
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def count_tokens(text):
    """Liczy tokeny w tek≈õcie u≈ºywajƒÖc tiktoken."""
    try:
        enc = tiktoken.get_encoding("cl100k_base")  # U≈ºywamy standardowego encodera
        tokens = enc.encode(text)
        return len(tokens)
    except Exception as e:
        print(f"B≈ÇƒÖd podczas liczenia token√≥w: {e}")
        return 0


def crawl_url(url):
    """Crawluje pojedynczy URL u≈ºywajƒÖc BeautifulSoup."""
    #print(f"\nCrawling: {url}")
    
    try:
        response = requests.get(
            url,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
            },
            timeout=30,
            allow_redirects=False
        )
        
        if response.status_code != 200:
            print(f"[SKIP] Status code: {response.status_code}")
            if 300 <= response.status_code < 400:
                print(f"  => Redirect to: {response.headers.get('location', 'unknown')}")
            return response.status_code, None
            
        # Po prostu dekodujemy z utf-8 ignorujƒÖc b≈Çƒôdy
        content = response.content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(content, 'html.parser')
        
        # Znajdujemy body
        body = soup.find('body')
        if not body:
            print(f"[ERROR] No body tag found")
            return None, None
            
        # Usuwamy niepotrzebne elementy
        for tag in body.find_all(['nav', 'footer', 'header', 'script', 'style', 'iframe', 'noscript']):
            tag.decompose()
            
        # WyciƒÖgamy ca≈Çy HTML z body
        html_content = str(body)
        
        # Konwertujemy HTML na Markdown
        converter = md(heading_style='ATX', strip_document='STRIP')
        markdown_content = converter.convert(html_content)
        
        # Czy≈õcimy tekst z nadmiarowych bia≈Çych znak√≥w
        markdown_content = ' '.join(markdown_content.split())  # Usuwa podw√≥jne spacje i znaki nowej linii
        
        #print(markdown_content)
        print(f"[OK] Successfully processed {url} ({len(markdown_content)} chars)")
        return response.status_code, markdown_content
        
    except Exception as e:
        print(f"[ERROR] Processing failed for {url}: {str(e)}")
        return None, None

def crawl_urls(urls):
    """Crawluje listƒô URLi."""
    crawled_pages = {}
    total_tokens = 0
    
    # Najpierw sprawdzamy cache
    for url in urls:
        if url in st.session_state.crawl_cache:
            text = st.session_state.crawl_cache[url]
            num_tokens = count_tokens(text)
            total_tokens += num_tokens
            print(f"[CACHE] Using cached content for {url} ({num_tokens} tokens)")
            crawled_pages[url] = text
            continue
            
        print(f"Crawling: {url}")
        status_code, text = crawl_url(url)
        
        if status_code == 200 and text:
            num_tokens = count_tokens(text)
            total_tokens += num_tokens
            st.session_state.crawl_cache[url] = text
            crawled_pages[url] = text
            print(f"[OK] {url} - {len(text)} chars, {num_tokens} tokens")
        else:
            print(f"[SKIP] {url} => Status code: {status_code}")
    
    print(f"\nSuccessfully crawled {len(crawled_pages)} pages with status 200 (from {len(urls)} total URLs)")
    print(f"Total tokens: {total_tokens}")
    return crawled_pages

#GO!
if 'embeddings_cache' not in st.session_state:
    st.session_state.embeddings_cache = {}

# Dodajmy unikalny identyfikator sesji
if 'session_id' not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

if st.sidebar.button("Wyczy≈õƒá cache embedding√≥w"):
    st.session_state.embeddings_cache = {}
    st.success("Cache zosta≈Ç wyczyszczony!")

def get_embeddings(text, provider="ollama"):
    """Get embeddings using selected provider."""
    cache_key = f"{st.session_state.session_id}_{text}"
    if cache_key in st.session_state.embeddings_cache:
        return st.session_state.embeddings_cache[cache_key]
    
    embedding = None
    
    try:
        provider = provider.lower()
        
        if provider == "ollama":
            response = requests.post(
                f"{st.session_state.host.rstrip('/')}/api/embed",
                json={
                    'model': st.session_state.selected_model,
                    'input': text
                }
            )
            response.raise_for_status()
            data = response.json()
            if 'embeddings' in data and len(data['embeddings']) > 0:
                embedding = np.array(data['embeddings'][0])
            
        elif provider == "openai":
            response = requests.post(
                "https://api.openai.com/v1/embeddings",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {st.session_state.api_keys['openai']}"
                },
                json={
                    "input": text,
                    "model": "text-embedding-3-small"
                }
            )
            response.raise_for_status()
            data = response.json()
            if 'data' in data and len(data['data']) > 0:
                embedding = np.array(data['data'][0]['embedding'])
        
        elif provider == "jina":
            response = requests.post(
                "https://api.jina.ai/v1/embeddings",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {st.session_state.api_keys['jina']}"
                },
                json={
                    "model": st.session_state.selected_model,
                    "dimensions": 1024,
                    "normalized": True,
                    "embedding_type": "float",
                    "input": [{"text": text}] if "clip" in st.session_state.selected_model else [text]
                }
            )
            response.raise_for_status()
            data = response.json()
            if 'data' in data and len(data['data']) > 0:
                embedding = np.array(data['data'][0]['embedding'])
        
        if embedding is not None:
            st.session_state.embeddings_cache[cache_key] = embedding
            return embedding
            
        return None
        
    except Exception as e:
        print(f"[ERROR] {provider} API error: {str(e)}")
        return None

def fetch_sitemap_urls_from_xml(sitemap_url, domain, recursive=False, processed_sitemaps=None, all_urls=None):
    """Fetch URLs from a sitemap XML file."""
    if processed_sitemaps is None:
        processed_sitemaps = set()
    if all_urls is None:
        all_urls = set()
    
    # Je≈õli ta sitemap by≈Ça ju≈º przetworzona, pomijamy
    if sitemap_url in processed_sitemaps:
        print(f"[SKIP] Ju≈º przetworzono: {sitemap_url}")
        return all_urls
    
    processed_sitemaps.add(sitemap_url)
    print(f"\n--- Przetwarzanie nowej sitemap: {sitemap_url} ---")
    
    try:
        response = requests.get(
            sitemap_url, 
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }, 
            timeout=10
        )
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "lxml-xml")
        
        if soup.find_all("sitemap"):
            print("Znaleziono zagnie≈ºd≈ºone sitemapy:")
            for sitemap in soup.find_all("sitemap"):
                loc = sitemap.find("loc")
                if loc:
                    nested_url = loc.text
                    if nested_url not in processed_sitemaps:
                        print(f"- {nested_url}")
                        if recursive:
                            fetch_sitemap_urls_from_xml(nested_url, domain, recursive=True, 
                                                      processed_sitemaps=processed_sitemaps, 
                                                      all_urls=all_urls)
        else:
            new_urls = 0
            for loc in soup.find_all("loc"):
                url = loc.text
                if not re.search(r"\.(jpg|jpeg|png|gif|svg|webp|bmp|tif|tiff)$", url, re.IGNORECASE):
                    if url not in all_urls:
                        all_urls.add(url)
                        new_urls += 1
            print(f"Dodano {new_urls} nowych URLi (≈ÇƒÖcznie: {len(all_urls)})")
            
    except Exception as e:
        print(f"B≈ÇƒÖd podczas przetwarzania {sitemap_url}: {str(e)}")
    
    return all_urls

def fetch_sitemap_urls(domain):
    """Fetch and parse URLs from sitemaps, excluding images and handling nested sitemaps."""
    domain = domain.replace("https://", "").replace("http://", "").strip("/")
    sitemap_urls = [
        f"https://{domain}/sitemap.xml",
        f"https://{domain}/sitemap_index.xml",
        f"https://{domain}/console/integration/execute/name/GoogleSitemap",
        f"https://{domain}/robots.txt"
    ]
    processed_sitemaps = set()
    all_urls = set()

    print(f"\nRozpoczynam przetwarzanie map witryny dla: {domain}")
    for sitemap_url in sitemap_urls:
        try:
            response = requests.get(sitemap_url, headers={"User-Agent": "SiteFocusTool/1.0"}, timeout=10)
            response.raise_for_status()
            if "robots.txt" in sitemap_url:
                for line in response.text.splitlines():
                    if line.lower().startswith("sitemap:"):
                        nested_sitemap_url = line.split(":", 1)[1].strip()
                        all_urls.update(fetch_sitemap_urls_from_xml(nested_sitemap_url, domain, 
                                                                  recursive=True, 
                                                                  processed_sitemaps=processed_sitemaps,
                                                                  all_urls=all_urls))
            else:
                all_urls.update(fetch_sitemap_urls_from_xml(sitemap_url, domain, 
                                                          recursive=True,
                                                          processed_sitemaps=processed_sitemaps,
                                                          all_urls=all_urls))
        except requests.RequestException:
            continue
            
    print(f"\nZnaleziono ≈ÇƒÖcznie {len(all_urls)} unikalnych URLi")
    return list(all_urls)

def clean_text_from_url(url, domain):
    """Clean URL by removing root domain and extracting readable text."""
    domain = domain.replace("https://", "").replace("http://", "").strip("/")
    url = url.replace(f"https://{domain}/", "").replace(f"http://{domain}/", "")
    text = re.sub(r"[^\w\s]", " ", url)
    text = text.replace("/", " ").replace("_", " ").replace("-", " ")
    return text.strip()

def calculate_site_focus_and_radius(embeddings):
    """Oblicza Site Focus Score i promie≈Ñ."""
    centroid = np.mean(embeddings, axis=0)
    deviations = np.array([
        1 - cosine_similarity(embedding.reshape(1, -1), centroid.reshape(1, -1))[0][0]
        for embedding in embeddings
    ])
    # Teraz deviations to "odleg≈Ço≈õci od centrum"
    return np.mean(deviations), np.std(deviations), centroid, deviations

def plot_gradient_strip_with_indicator(score, title):
    """Visualize the score as a gradient strip with an indicator."""
    plt.figure(figsize=(8, 1))
    gradient = np.linspace(0, 1, 256).reshape(1, -1)
    gradient = np.vstack((gradient, gradient))
    plt.imshow(gradient, aspect="auto", cmap="RdYlGn_r")
    plt.axvline(x=score * 256, color="black", linestyle="--", linewidth=2)
    plt.gca().set_axis_off()
    plt.title(f"{title}: {score * 100:.2f}%")
    st.pyplot(plt)

def plot_3d_tsne(embeddings, urls, centroid, deviations):
    """Interactive 3D t-SNE scatter plot with hover labels."""
    tsne = TSNE(n_components=3, random_state=42, perplexity=min(30, len(embeddings) - 1))
    tsne_results = tsne.fit_transform(np.vstack([embeddings, centroid]))
    centroid_tsne = tsne_results[-1]
    tsne_results = tsne_results[:-1]

    fig = px.scatter_3d(
        x=tsne_results[:, 0],
        y=tsne_results[:, 1],
        z=tsne_results[:, 2],
        color=deviations,
        color_continuous_scale="RdYlGn_r",
        hover_name=urls,
        labels={"color": "Deviation"},
        title="3D t-SNE Projection of Page Embeddings"
    )
    fig.add_scatter3d(
        x=[centroid_tsne[0]],
        y=[centroid_tsne[1]],
        z=[centroid_tsne[2]],
        mode="markers",
        marker=dict(size=15, color="green"),
        name="Centroid"
    )
    st.plotly_chart(fig)

def plot_spherical_distances_optimized(deviations, embeddings, urls):
    """Improved scatter plot showing distances in a spherical layout."""
    num_points = len(deviations)
    angles = np.linspace(0, 2 * np.pi, num_points, endpoint=False)

    fig = px.scatter_polar(
        r=deviations,
        theta=np.degrees(angles),
        color=deviations,
        color_continuous_scale="RdYlGn_r",
        title="Optimized Spherical Plot of Page Distances from Centroid",
        labels={"color": "Deviation"}
    )
    fig.update_traces(
        mode="markers",
        hovertemplate="%{text}<extra></extra>",
        text=urls
    )
    st.plotly_chart(fig)

def analyze_thematic_center(valid_urls, deviations, embeddings):
    """Analyze thematic center of the website."""
    st.header("üéØ Analiza centrum tematycznego")
    
    # 1. Podsumowanie statystyczne
    st.subheader("üìä Statystyki Centrum")
    col1, col2, col3 = st.columns(3)
    
    mean_distance = np.mean([1 - dev for dev in deviations])
    with col1:
        st.metric("≈örednia odleg≈Ço≈õƒá od centrum", f"{mean_distance:.3f}")
    with col2:
        st.metric("Liczba stron", f"{len(valid_urls)}")
    with col3:
        st.metric("Odchylenie standardowe", f"{np.std([1 - dev for dev in deviations]):.3f}")
    
    # 2. Wykres dystrybucji
    st.subheader("üìà Rozk≈Çad odleg≈Ço≈õci od centrum")
    plot_distance_distribution(deviations)
    
    # 3. Lista WSZYSTKICH stron z odleg≈Ço≈õciami
    st.subheader("Lista stron")
    st.markdown("""
    Lista wszystkich stron wraz z ich odleg≈Ço≈õciƒÖ od centrum tematycznego.
    Im wy≈ºszy procent, tym bardziej strona jest reprezentatywna dla g≈Ç√≥wnego tematu.
    """)
    
    # Tworzymy DataFrame ze WSZYSTKIMI stronami
    all_pages_df = pd.DataFrame({
        "URL": valid_urls,
        "Blisko≈õƒá do centrum": [1 - dev for dev in deviations]
    })
    
    # Formatujemy procenty i sortujemy malejƒÖco
    all_pages_df["Blisko≈õƒá do centrum"] = all_pages_df["Blisko≈õƒá do centrum"].apply(lambda x: f"{x*100:.1f}%")
    all_pages_df = all_pages_df.sort_values("Blisko≈õƒá do centrum", ascending=False)
    
    # Dodajemy numeracjƒô
    all_pages_df.index = range(1, len(all_pages_df) + 1)
    all_pages_df.index.name = "Rank"
    
    # Wy≈õwietlamy ca≈Çy DataFrame
    st.dataframe(all_pages_df)
    
    # 4. Wskaz√≥wki interpretacyjne
    st.markdown("""
    ### üí° Jak interpretowaƒá wyniki:
    
    1. **Strony centralne** - Strony z najwy≈ºszym procentem blisko≈õci do centrum najlepiej reprezentujƒÖ g≈Ç√≥wny temat witryny
    2. **Rozk≈Çad odleg≈Ço≈õci** - Wykres pokazuje, jak rozproszone sƒÖ strony wok√≥≈Ç centrum tematycznego:
        - WƒÖski rozk≈Çad ‚Üí Strona jest bardzo sp√≥jna tematycznie
        - Szeroki rozk≈Çad ‚Üí Strona pokrywa r√≥≈ºnorodne tematy
    3. **≈örednia odleg≈Ço≈õƒá** - Im bli≈ºsza 1, tym bardziej sp√≥jna tematycznie jest strona
    """)

# Dodajemy nowƒÖ funkcjƒô do obliczania odleg≈Ço≈õci od URL referencyjnego
def calculate_distances_from_reference(reference_embedding, embeddings, reference_url, valid_urls):
    """Calculate distances from reference URL embedding to all other embeddings."""
    distances = []
    urls = []
    
    # Dodajemy sam URL referencyjny z odleg≈Ço≈õciƒÖ 0
    distances.append(0)
    urls.append(reference_url)
    
    # Obliczamy odleg≈Ço≈õci dla pozosta≈Çych URLi
    for emb, url in zip(embeddings, valid_urls):
        if url != reference_url:  # Pomijamy URL referencyjny w pƒôtli
            similarity = np.dot(reference_embedding, emb)
            distance = 1 - similarity
            distances.append(distance)
            urls.append(url)
            
    return np.array(distances), urls

def collect_closest_pages(domain, urls, distances, n=10):
    """Collect n closest pages from each domain."""
    df = pd.DataFrame({
        "Domain": [domain] * len(urls),
        "URL": urls,
        "Distance": distances
    })
    return df.nlargest(n, "Distance")

def plot_spherical_reference_comparison(reference_url, domains_data):
    """Create spherical plot with reference URL in center and closest pages around."""
    fig = go.Figure()
    
    # Dodajemy punkt centralny (URL referencyjny)
    fig.add_trace(go.Scatter3d(
        x=[0],
        y=[0],
        z=[0],
        mode='markers+text',
        marker=dict(size=10, color='red'),
        text=[reference_url],
        name='Reference URL'
    ))
    
    # Generujemy punkty na sferze dla ka≈ºdej domeny
    colors = px.colors.qualitative.Set3  # R√≥≈ºne kolory dla r√≥≈ºnych domen
    for idx, (domain, df) in enumerate(domains_data.groupby("Domain")):
        # Generujemy punkty na sferze
        n_points = len(df)
        phi = np.linspace(0, 2*np.pi, n_points)
        theta = np.linspace(-np.pi/2, np.pi/2, n_points)
        
        # Konwertujemy odleg≈Ço≈õci na promie≈Ñ (im mniejsza odleg≈Ço≈õƒá, tym bli≈ºej centrum)
        r = 1 + df["Distance"].values
        
        x = r * np.cos(theta) * np.cos(phi)
        y = r * np.cos(theta) * np.sin(phi)
        z = r * np.sin(theta)
        
        fig.add_trace(go.Scatter3d(
            x=x, y=y, z=z,
            mode='markers+text',
            marker=dict(size=6, color=colors[idx % len(colors)]),
            text=df["URL"],
            name=domain,
            hovertemplate="Domain: %{text}<br>Distance: %{customdata}<extra></extra>",
            customdata=df["Distance"]
        ))
    
    fig.update_layout(
        title="Spherical Distribution of Closest Pages to Reference URL",
        scene=dict(
            xaxis_title="X",
            yaxis_title="Y",
            zaxis_title="Z"
        ),
        showlegend=True
    )
    
    st.plotly_chart(fig)

def collect_cross_domain_analysis(reference_embedding, domain_results, n=10):
    """Analyze results across all domains with reference URL as center."""
    all_results = []
    
    for domain_data in domain_results:
        domain = domain_data['domain']
        embeddings = domain_data['embeddings']
        urls = domain_data['urls']
        
        # Obliczamy odleg≈Ço≈õci od URL referencyjnego
        distances = []
        for emb in embeddings:
            similarity = np.dot(reference_embedding, emb)
            distance = 1 - similarity
            distances.append(distance)
        
        # Zbieramy n najbli≈ºszych stron
        closest_indices = np.argsort(distances)[:n]
        for idx in closest_indices:
            all_results.append({
                'Domain': domain,
                'URL': urls[idx],
                'Distance': distances[idx]
            })
    
    return pd.DataFrame(all_results)

def plot_distance_distribution(deviations):
    """Plot distribution of distances from center."""
    fig = go.Figure()
    
    # Konwertujemy deviacje na procenty blisko≈õci do centrum
    proximities = [(1 - dev) * 100 for dev in deviations]
    
    # Dodajemy histogram
    fig.add_trace(go.Histogram(
        x=proximities,
        nbinsx=30,
        name='Rozk≈Çad',
        hovertemplate="Blisko≈õƒá do centrum: %{x:.1f}%<br>Liczba stron: %{y}<extra></extra>"
    ))
    
    # Dodajemy liniƒô ≈õredniej
    mean_proximity = np.mean(proximities)
    fig.add_vline(
        x=mean_proximity,
        line_dash="dash",
        line_color="red",
        annotation_text=f"≈örednia: {mean_proximity:.1f}%",
        annotation_position="top"
    )
    
    # Aktualizujemy layout
    fig.update_layout(
        title="Rozk≈Çad odleg≈Ço≈õci stron od centrum tematycznego",
        xaxis_title="Blisko≈õƒá do centrum (%)",
        yaxis_title="Liczba stron",
        showlegend=False,
        bargap=0.1
    )
    
    # Wy≈õwietlamy wykres
    st.plotly_chart(fig)

def plot_2d_tsne(embeddings, urls, centroid, deviations):
    """2D t-SNE scatter plot with hover labels."""
    # Dodajemy centroid do embeddings
    all_embeddings = np.vstack([embeddings, centroid])
    
    # Wykonujemy t-SNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings) - 1))
    tsne_results = tsne.fit_transform(all_embeddings)
    
    # Oddzielamy wyniki dla stron i centroidu
    pages_tsne = tsne_results[:-1]
    centroid_tsne = tsne_results[-1]
    
    # Tworzymy DataFrame dla plotly
    df = pd.DataFrame({
        'x': pages_tsne[:, 0],
        'y': pages_tsne[:, 1],
        'Distance': deviations,
        'URL': urls
    })
    
    # Tworzymy wykres
    fig = px.scatter(
        df,
        x='x',
        y='y',
        color='Distance',
        color_continuous_scale='RdYlGn_r',
        hover_data=['URL'],
        title='2D t-SNE Projection of Pages',
        labels={'x': 't-SNE 1', 'y': 't-SNE 2'}
    )
    
    # Dodajemy centroid
    fig.add_trace(
        go.Scatter(
            x=[centroid_tsne[0]],
            y=[centroid_tsne[1]],
            mode='markers',
            marker=dict(
                color='red',
                symbol='star',
                size=20
            ),
            name='Centroid',
            hoverinfo='name'
        )
    )
    
    # Aktualizujemy layout
    fig.update_layout(
        width=800,
        height=600,
        showlegend=True
    )
    
    # Wy≈õwietlamy wykres
    st.plotly_chart(fig)

def get_focus_score_interpretation(score):
    """Zwraca interpretacjƒô dla Site Focus Score."""
    if score < 0.30:
        return "üî¥ Niska sp√≥jno≈õƒá tematyczna - strona porusza wiele r√≥≈ºnych temat√≥w"
    elif score < 0.60:
        return "üü° ≈örednia sp√≥jno≈õƒá tematyczna - strona ma kilka g≈Ç√≥wnych obszar√≥w tematycznych"
    else:
        return "üü¢ Wysoka sp√≥jno≈õƒá tematyczna - strona jest mocno skoncentrowana na jednym temacie"

def get_radius_interpretation(radius):
    """Zwraca interpretacjƒô dla Site Radius."""
    if radius < 0.15:
        return "üü¢ Ma≈Çe rozproszenie - tre≈õci sƒÖ bardzo sp√≥jne ze sobƒÖ"
    elif radius < 0.30:
        return "üü° ≈örednie rozproszenie - tre≈õci sƒÖ umiarkowanie zr√≥≈ºnicowane"
    else:
        return "üî¥ Du≈ºe rozproszenie - tre≈õci sƒÖ bardzo zr√≥≈ºnicowane"

def split_into_chunks(text, provider="ollama", max_tokens=500, overlap=50):
    """Dzieli tekst na chunki z r√≥≈ºnymi limitami dla r√≥≈ºnych dostawc√≥w."""
    enc = tiktoken.get_encoding("cl100k_base")
    tokens = enc.encode(text)
    total_tokens = len(tokens)
    
    # Ustawiamy limit token√≥w w zale≈ºno≈õci od dostawcy
    if provider == "openai" or provider == "jina":
        max_tokens = 8000
        overlap = 100  # Wiƒôkszy overlap dla wiƒôkszych chunk√≥w
    
    chunks = []
    for i in range(0, len(tokens), max_tokens - 2*overlap):
        start = max(0, i - overlap)
        end = min(len(tokens), i + max_tokens - overlap)
        chunk = tokens[start:end]
        if chunk:
            chunks.append(enc.decode(chunk))
    
    print(f"[TOKENS] Tekst zawiera {total_tokens} token√≥w")
    return chunks

def get_averaged_embedding(text, provider="ollama"):
    """Generuje u≈õredniony embedding z chunk√≥w tekstu."""
    chunks = split_into_chunks(text, provider=provider)
    print(f"[CHUNKS] Podzielono tekst na {len(chunks)} chunk√≥w")
    chunk_embeddings = []
    
    for i, chunk in enumerate(chunks, 1):
        print(f"[CHUNK {i}/{len(chunks)}] Generujƒô embedding...")
        embedding = get_embeddings(chunk, provider=provider)
        if embedding is not None:
            chunk_embeddings.append(embedding)
    
    if not chunk_embeddings:
        return None
        
    averaged_embedding = np.mean(chunk_embeddings, axis=0)
    averaged_embedding = averaged_embedding / np.linalg.norm(averaged_embedding)
    
    return averaged_embedding

# Streamlit Interface
st.title("SiteFocus Tool")

# Sprawdzenie dostƒôpno≈õci Ollamy
try:
    response = requests.get('http://localhost:11434/api/tags')
    ollama_available = response.status_code == 200
    if ollama_available:
        st.success("‚úÖ Wykryto lokalnƒÖ instalacjƒô Ollamy")
except requests.exceptions.ConnectionError:
    ollama_available = False
    st.warning("‚ö†Ô∏è Nie wykryto lokalnej instalacji Ollamy")

# Wyb√≥r dostawcy embedding√≥w
col1, col2, col3 = st.columns([1, 1, 1])

with col1:
    provider = st.radio(
        "Wybierz dostawcƒô:",
        options=["Ollama", "OpenAI", "Jina"],
        index=0,
    ).lower()

with col2:
    if provider == "ollama":
        st.session_state.host = st.text_input(
            "Host:",
            value="http://localhost:11434/",
            help="Domy≈õlnie: http://localhost:11434/"
        )
    elif provider == "openai":
        st.session_state.host = "https://api.openai.com/v1/"
        st.write(f"Endpoint: {st.session_state.host}")
    elif provider == "jina":
        st.session_state.host = "https://api.jina.ai/v1/"
        st.write(f"Endpoint: {st.session_state.host}")

with col3:
    if provider in ["openai", "jina"]:
        api_key = st.text_input(
            "Klucz API:",
            type="password",
            value=st.session_state.api_keys.get(provider),
            help=f"Wprowad≈∫ klucz API dla {provider.upper()}"
        )
        if api_key:
            st.session_state.api_keys[provider] = api_key
    else:
        st.write("API: -")

# Guzik do pobierania modeli
if st.button("Pobierz modele"):
    print(f"\n--- Pobieranie modeli dla {provider} ---")
    
    # Reset poprzedniego wyboru modelu
    st.session_state.selected_model = None
    st.session_state.model_initialized = False
    
    if provider == "ollama":
        try:
            host_url = st.session_state.host.rstrip('/')
            print(f"Pr√≥ba po≈ÇƒÖczenia z Ollama na URL: {host_url}")
            response = requests.get(f'{host_url}/api/tags')
            print(f"Status odpowiedzi: {response.status_code}")
            
            if response.status_code == 200:
                models = response.json()
                if models.get('models'):
                    st.session_state.model_list = [model['name'] for model in models['models']]
                    print(f"Znaleziono modele: {st.session_state.model_list}")
                    st.success("Pobrano listƒô modeli")
                else:
                    st.session_state.model_list = []
                    print("Brak modeli w odpowiedzi")
                    st.info("Brak dostƒôpnych modeli")
            else:
                st.error(f"B≈ÇƒÖd podczas pobierania modeli: {response.status_code}")
        except Exception as e:
            st.error(f"B≈ÇƒÖd podczas pobierania modeli Ollama: {str(e)}")
            
    elif provider == "openai":
        if not st.session_state.api_keys.get('openai'):
            st.error("Wprowad≈∫ klucz API OpenAI")
        else:
            st.session_state.model_list = ["text-embedding-3-small", "text-embedding-3-large"]
            st.success("Pobrano listƒô modeli")
            
    elif provider == "jina":
        if not st.session_state.api_keys.get('jina'):
            st.error("Wprowad≈∫ klucz API Jina")
        else:
            st.session_state.model_list = ["jina-clip-v2", "jina-embeddings-v3"]
            st.success("Pobrano listƒô modeli")

# W miejscu gdzie wybieramy model
if st.session_state.model_list:
    if not st.session_state.model_initialized or st.session_state.selected_model not in st.session_state.model_list:
        # Inicjalizujemy model tylko raz lub gdy poprzedni model nie jest dostƒôpny
        st.session_state.selected_model = st.session_state.model_list[0]
        st.session_state.model_initialized = True
    
    # Wy≈õwietlamy selectbox z aktualnie wybranym modelem
    selected = st.selectbox(
        "Wybierz model:", 
        st.session_state.model_list,
        index=st.session_state.model_list.index(st.session_state.selected_model)
    )
    
    # Aktualizujemy model tylko je≈õli u≈ºytkownik faktycznie zmieni≈Ç wyb√≥r
    if selected != st.session_state.selected_model:
        st.session_state.selected_model = selected
        print(f"Zmieniono model na: {selected}")

# Debug mode toggle
if 'debug' not in st.session_state:
    st.session_state.debug = False
st.sidebar.checkbox('Debug Mode', value=False, key='debug')

# Najpierw pole URL referencyjnego
reference_url = st.text_input(
    "URL referencyjny (opcjonalnie):", 
    placeholder="https://example.com/page",
    help="Je≈õli nie podasz URL referencyjnego, analiza bƒôdzie przeprowadzona wzglƒôdem centrum tematycznego wszystkich stron."
)

# Nastƒôpnie pole na domeny
domains = st.text_area(
    "Wprowad≈∫ domeny (ka≈ºda w nowej linii):", 
    placeholder="example.com\nexample2.com\nexample3.com",
    help="Wprowadz jednƒÖ lub wiƒôcej domen, ka≈ºdƒÖ w nowej linii"
)

# Inicjalizacja reference_embedding
reference_embedding = None

st.markdown("""
    ‚ÑπÔ∏è **Blisko≈õƒá do centrum** - miara pokazujƒÖca jak blisko centrum tematycznego znajduje siƒô dana strona:
    - Wy≈ºsza warto≈õƒá = strona jest bardziej zgodna z g≈Ç√≥wnƒÖ tematykƒÖ domeny
    - Ni≈ºsza warto≈õƒá = strona bardziej odbiega od typowej tre≈õci na stronie
""")


if st.button("START"):
    # Sprawdzamy konfiguracjƒô na poczƒÖtku
    if provider == "ollama" and not st.session_state.selected_model:
        if st.session_state.model_cache:
            st.session_state.selected_model = st.session_state.model_cache
        else:
            st.error("Nie wybrano modelu! Proszƒô kliknƒÖƒá 'Pobierz modele' i wybraƒá model.")
            st.stop()
    elif provider == "openai" and not st.session_state.api_keys.get('openai'):
        st.error("Nie podano klucza API OpenAI!")
        st.stop()
    elif provider == "jina" and not st.session_state.api_keys.get('jina'):
        st.error("Nie podano klucza API Jina!")
        st.stop()

    if domains:
        # Najpierw crawlujemy URL referencyjny (je≈õli podany)
        if reference_url:
            ref_crawled = crawl_urls([reference_url])
            if ref_crawled and reference_url in ref_crawled:
                reference_text = ref_crawled[reference_url]
                # Zapisz do pliku nadpisujƒÖc poprzedniƒÖ zawarto≈õƒá
                reference_file = f"reference_text_{st.session_state.session_id}.txt"
                with open(reference_file, "w", encoding="utf-8") as f:
                    f.write(reference_text)
                print(reference_text)
                reference_embedding = get_averaged_embedding(reference_text, provider=provider)
                if reference_embedding is not None:
                    reference_embedding = reference_embedding / norm(reference_embedding)
                    st.success("Pomy≈õlnie wygenerowano embedding dla URL referencyjnego")
                else:
                    st.error("Nie uda≈Ço siƒô wygenerowaƒá embeddingu dla URL referencyjnego")
                    st.stop()
            else:
                st.error("Failed to crawl reference URL")
                st.stop()
        
        # Dzielimy tekst na listƒô domen i usuwamy puste linie
        domain_list = [d.strip() for d in domains.split('\n') if d.strip()]
        
        # Na samym poczƒÖtku
        all_results = []  # Lista na wszystkie wyniki
        
        # G≈Ç√≥wna pƒôtla po domenach
        for domain in domain_list:
            st.subheader(f"Analiza domeny: {domain}")
            print(f"Rozpoczƒôcie przetwarzania domeny: {domain}")
            
            with st.spinner(f"Fetching URLs for {domain}..."):
                urls = fetch_sitemap_urls(domain)
                if not urls:
                    st.error(f"No URLs found for {domain}")
                    continue
                    
                st.info(f"Found {len(urls)} URLs for {domain}")
                
                # Sprawdzamy statusy i crawlujemy od razu strony z 200
                crawled_pages = crawl_urls(urls)
                if not crawled_pages:
                    st.warning(f"No valid pages found from {len(urls)} URLs")
                    continue
                    
                valid_urls = list(crawled_pages.keys())
                texts = list(crawled_pages.values())
                st.info(f"Successfully crawled {len(crawled_pages)} pages with status 200 (from {len(urls)} total URLs)")
                
                # Generowanie embedding√≥w
                embeddings = []
                valid_urls = []  # Resetujemy listƒô

                for url, text in crawled_pages.items():
                    print(f"Generujƒô embedding dla URL: {url}")
                    embedding = get_averaged_embedding(text, provider=provider)
                    if embedding is not None:
                        embeddings.append(embedding)
                        valid_urls.append(url)
                
                # Najpierw obliczamy wyniki
                if embeddings:
                    st.success(f"Successfully generated {len(embeddings)} embeddings with model {st.session_state.selected_model}")
                    
                    # Najpierw obliczamy wyniki
                    embeddings = np.array(embeddings)
                    site_focus_score, site_radius, centroid, deviations = calculate_site_focus_and_radius(embeddings)
                    
                    # Potem zapisujemy do cache
                    st.session_state.analysis_results[domain] = {
                        'embeddings': embeddings,
                        'valid_urls': valid_urls,
                        'site_focus_score': site_focus_score,
                        'site_radius': site_radius,
                        'centroid': centroid,
                        'deviations': deviations
                    }
                    
                    # Wy≈õwietlamy wyniki
                    st.subheader("üìä Metryki sp√≥jno≈õci tematycznej")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric(label="Site Focus Score", value=f"{site_focus_score:.2%}")
                        st.markdown(get_focus_score_interpretation(site_focus_score))
                        st.markdown("""
                        **Site Focus Score (Sp√≥jno≈õƒá tematyczna)**:

                        - **<30%** - Niska sp√≥jno≈õƒá tematyczna  
                        - Strona porusza wiele r√≥≈ºnych, niepowiƒÖzanych temat√≥w  
                        - Tre≈õci sƒÖ bardzo zr√≥≈ºnicowane  
                        - Typowe dla portali og√≥lnotematycznych lub agregator√≥w tre≈õci  

                        - **30-60%** - ≈örednia sp√≥jno≈õƒá tematyczna  
                        - Strona ma kilka g≈Ç√≥wnych obszar√≥w tematycznych  
                        - Tre≈õci sƒÖ powiƒÖzane, ale zr√≥≈ºnicowane  
                        - Typowe dla portali bran≈ºowych lub blog√≥w o szerokiej tematyce  

                        - **>60%** - Wysoka sp√≥jno≈õƒá tematyczna  
                        - Strona koncentruje siƒô na jednym g≈Ç√≥wnym temacie  
                        - Tre≈õci sƒÖ ≈õci≈õle ze sobƒÖ powiƒÖzane  
                        - Typowe dla specjalistycznych stron i blog√≥w tematycznych  
                        """)
                        
                    with col2:
                        st.metric(label="Site Radius", value=f"{site_radius:.2%}")
                        st.markdown(get_radius_interpretation(site_radius))
                        st.markdown("""
                                    **Site Radius (Rozproszenie tre≈õci)**:

                                    - **<15%** - Ma≈Çe rozproszenie  
                                    - Tre≈õci sƒÖ bardzo sp√≥jne ze sobƒÖ  
                                    - Poszczeg√≥lne strony trzymajƒÖ siƒô g≈Ç√≥wnego tematu  
                                    - Wskazuje na konsekwentnƒÖ strategiƒô tre≈õci  

                                    - **15-30%** - ≈örednie rozproszenie  
                                    - Tre≈õci sƒÖ umiarkowanie zr√≥≈ºnicowane  
                                    - WystƒôpujƒÖ odstƒôpstwa od g≈Ç√≥wnego tematu  
                                    - Typowe dla stron z r√≥≈ºnorodnymi podsekcjami  

                                    - **>30%** - Du≈ºe rozproszenie  
                                    - Tre≈õci znaczƒÖco r√≥≈ºniƒÖ siƒô od siebie  
                                    - Du≈ºe odchylenia od g≈Ç√≥wnego tematu  
                                    - Mo≈ºe wskazywaƒá na brak sp√≥jnej strategii tre≈õci  
                                    """)
                    # Dodajemy szczeg√≥≈Çowe wyja≈õnienie skali
                    st.markdown("""üí° **Optymalne warto≈õci** zale≈ºƒÖ od typu strony i jej przeznaczenia. Dla wyspecjalizowanego bloga tematycznego korzystne bƒôdƒÖ wysokie warto≈õci Site Focus Score i niskie Site Radius. Dla portalu informacyjnego naturalne bƒôdƒÖ ≈õrednie warto≈õci obu metryk.
                    """)
                    
                    # 2. Szczeg√≥≈Çowe wizualizacje z opisami
                    st.markdown("---")
                    st.subheader("siteFocusScore")
                    st.markdown(""" 
                    **Site Focus Score** odzwierciedla, jak mocno tre≈õci na stronie sƒÖ skupione wok√≥≈Ç jednego obszaru tematycznego.  
                    Wy≈ºszy wynik oznacza wiƒôkszƒÖ sp√≥jno≈õƒá tematycznƒÖ.
                    """)
                    
                    st.markdown(f"""
                        <div style='margin: 20px 0;'>
                            <p style='text-align: center; font-size: 1.2em;'>siteFocusScore: {site_focus_score:.2%}</p>
                            <div style='background: linear-gradient(to right, #00ff00, #ffff00, #ff0000); height: 30px; position: relative; border-radius: 4px;'>
                                <div style='position: absolute; left: {site_focus_score*100}%; border-left: 2px dashed black; height: 100%;'></div>
                            </div>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown("---")
                    st.subheader("siteRadius")
                    st.markdown("""
                    **Site Radius** mierzy, jak bardzo poszczeg√≥lne strony odbiegajƒÖ od g≈Ç√≥wnego tematu strony.  
                    Mniejszy promie≈Ñ oznacza wiƒôkszƒÖ sp√≥jno≈õƒá tre≈õci.
                    """)
                    
                    st.markdown(f"""
                        <div style='margin: 20px 0;'>
                            <p style='text-align: center; font-size: 1.2em;'>siteRadius: {site_radius:.2%}</p>
                            <div style='background: linear-gradient(to right, #00ff00, #ffff00, #ff0000); height: 30px; position: relative; border-radius: 4px;'>
                                <div style='position: absolute; left: {site_radius*100}%; border-left: 2px dashed black; height: 100%;'></div>
                            </div>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    # Dodajemy 2D t-SNE visualization
                    st.subheader("2D t-SNE Projection")
                    plot_2d_tsne(embeddings, valid_urls, centroid, deviations)
                    
                    # Je≈õli mamy URL referencyjny, dodajemy podsumowanie najbli≈ºszych stron
                    if reference_embedding is not None:
                        st.subheader("üéØ Top 10 stron najbli≈ºszych URL referencyjnemu")
                        
                        # Obliczamy odleg≈Ço≈õci od URL referencyjnego
                        distances_from_ref = []
                        for emb in embeddings:
                            similarity = np.dot(reference_embedding, emb)
                            distance = 1 - similarity
                            distances_from_ref.append(distance)
                        
                        # Tworzymy DataFrame z wynikami
                        domain_results_df = pd.DataFrame({
                            'URL': valid_urls,
                            'Distance': distances_from_ref
                        })
                        
                        # Sortujemy i bierzemy top 10
                        domain_top_10 = domain_results_df.nsmallest(10, 'Distance')
                        
                        # Dodajemy numeracjƒô
                        domain_top_10.index = range(1, len(domain_top_10) + 1)
                        domain_top_10.index.name = "Rank"
                        
                        # Formatujemy odleg≈Ço≈õci na procenty podobie≈Ñstwa
                        domain_top_10['Similarity'] = domain_top_10['Distance'].apply(lambda x: f"{(1-x)*100:.1f}%")
                        
                        # Wy≈õwietlamy tabelƒô
                        st.dataframe(domain_top_10[['URL', 'Similarity']])
                        
                        # Dodajemy kr√≥tkie podsumowanie
                        mean_similarity = (1 - np.mean(distances_from_ref)) * 100
                        st.markdown(f"""
                        **Podsumowanie dla domeny {domain}:**
                        - ≈örednie podobie≈Ñstwo do URL referencyjnego: {mean_similarity:.1f}%
                        - Liczba przeanalizowanych stron: {len(valid_urls)}
                        """)
                    
                    # Analiza centrum tematycznego
                    analyze_thematic_center(valid_urls, deviations, embeddings)
                    
                    # Wizualizacje 3D
                    st.header("üåê Wizualizacje 3D")
                    st.subheader("3D t-SNE Projection")
                    plot_3d_tsne(embeddings, valid_urls, centroid, deviations)
                    
                    st.subheader("Spherical Distance Plot")
                    plot_spherical_distances_optimized(deviations, embeddings, valid_urls)
                    
                    # Zbieranie danych do analizy cross-domain
                    if reference_embedding is not None:
                        for url, emb in zip(valid_urls, embeddings):
                            similarity = np.dot(reference_embedding, emb)
                            distance = 1 - similarity
                            all_results.append({
                                'Domain': domain,
                                'URL': url,
                                'Distance': distance,
                                'Embedding': emb
                            })

        # CA≈ÅKOWICIE POZA PƒòTLƒÑ - analiza cross-domain
        if reference_url and len(all_results) > 0:
            st.header("üåç Por√≥wnanie domen", anchor="porownanie-domen")
            st.subheader("Najbli≈ºsze strony wzglƒôdem URL referencyjnego")
            
            # Tworzymy DataFrame ze wszystkich wynik√≥w
            cross_domain_results = pd.DataFrame(all_results)
            
            # Przekszta≈Çcamy reference_embeddings na w≈Ça≈õciwy kszta≈Çt
            if len(reference_embedding.shape) == 1:
                reference_embedding = reference_embedding.reshape(1, -1)
            reference_centroid = reference_embedding
            
            # Przeliczamy odleg≈Ço≈õci dla wszystkich URLi
            url_distances = {}
            for _, row in cross_domain_results.iterrows():
                url = row['URL']
                embeddings = np.array(row['Embedding'])
                if len(embeddings.shape) == 1:
                    embeddings = embeddings.reshape(1, -1)
                distance = 1 - cosine_similarity(reference_centroid, embeddings)[0][0]
                url_distances[url] = distance
            
            # Aktualizujemy DataFrame o nowe odleg≈Ço≈õci
            cross_domain_results['New_Distance'] = cross_domain_results['URL'].map(url_distances)
            
            # Sortujemy po nowych odleg≈Ço≈õciach
            results_table = (cross_domain_results
                .sort_values('New_Distance')
                .groupby('Domain')
                .head(10)
                .reset_index(drop=True))
            
            # Tworzymy wiersz dla URL referencyjnego
            reference_row = pd.DataFrame([{
                'Domain': urlparse(reference_url).netloc,
                'URL': reference_url,
                'New_Distance': 0.0
            }])
            
            # ≈ÅƒÖczymy URL referencyjny z wynikami
            results_table = pd.concat([reference_row, results_table], ignore_index=True)
            
            # Wy≈õwietlamy tabelƒô z nowymi odleg≈Ço≈õciami
            st.dataframe(
                results_table[['Domain', 'URL', 'New_Distance']],
                column_config={
                    "Domain": "Domena",
                    "URL": "Adres URL",
                    "New_Distance": st.column_config.NumberColumn(
                        "Odleg≈Ço≈õƒá od centroidu",
                        format="%.3f",
                    )
                },
                hide_index=True
            )
            
            # Dodajemy wykres polarny u≈ºywajƒÖc zoptymalizowanej funkcji
            st.markdown("---")
            st.subheader("üéØ Optimized Spherical Plot of Page Distances from Centroid")
            
            plot_spherical_distances_optimized(
                deviations=results_table['New_Distance'].values,
                embeddings=None,  # Nie potrzebujemy embedding√≥w do tego wykresu
                urls=results_table['URL'].values
            )

            st.markdown("""
            ### üéØ Interpretacja wykresu polarnego:
            - ≈örodek wykresu reprezentuje URL referencyjny (odleg≈Ço≈õƒá = 0)
            - Odleg≈Ço≈õƒá od ≈õrodka pokazuje r√≥≈ºnicƒô tematycznƒÖ:
                * Bli≈ºej ≈õrodka = tre≈õƒá bardziej podobna do referencyjnej
                * Dalej od ≈õrodka = wiƒôksza r√≥≈ºnica w tre≈õci
            - Kolor punkt√≥w reprezentuje odleg≈Ço≈õƒá (zielony = blisko, czerwony = daleko)
            """)

        # Po wygenerowaniu embedding√≥w
        if domain in st.session_state.analysis_results:
            results = st.session_state.analysis_results[domain]
            # Wy≈õwietlamy zapisane wyniki...
            st.success(f"Using cached results for {domain} with {len(results['embeddings'])} embeddings")
            # ... reszta wy≈õwietlania wynik√≥w ...

    # Czyszczenie po zako≈Ñczeniu
    def cleanup():
        if os.path.exists(reference_file):
            os.remove(reference_file)
    atexit.register(cleanup)

if st.sidebar.button("Wyczy≈õƒá cache wynik√≥w"):
    st.session_state.analysis_results = {}
    st.success("Cache wynik√≥w zosta≈Ç wyczyszczony")

# Zamiast tego mo≈ºemy dodaƒá informacjƒô o aktualnej sesji
st.sidebar.info(f"ID sesji: {st.session_state.session_id[:8]}...")

if st.sidebar.button("Wyczy≈õƒá klucze API"):
    st.session_state.api_keys = {
        'openai': None,
        'jina': None
    }
    st.success("Klucze API zosta≈Çy wyczyszczone!")

