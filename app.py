import streamlit as st
from streamlit.components.v1 import html
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.manifold import TSNE
from numpy.linalg import norm
import requests
from bs4 import BeautifulSoup
import re
import matplotlib.pyplot as plt
import io
from sklearn.metrics.pairwise import cosine_similarity
from urllib.parse import urlparse

#GO!

if 'embeddings_cache' not in st.session_state:
    st.session_state.embeddings_cache = {}

if st.sidebar.button("Wyczy≈õƒá cache embedding√≥w"):
    st.session_state.embeddings_cache = {}
    st.success("Cache zosta≈Ç wyczyszczony!")

@st.cache_data
def get_embeddings(text):
    """Get embeddings using Ollama API with caching"""
    # Sprawdzamy cache
    cache_key = text
    if cache_key in st.session_state.embeddings_cache:
        print(f"U≈ºywam embeddingu z cache dla tekstu: {text[:100]}...")
        return st.session_state.embeddings_cache[cache_key]
    
    try:
        print(f"Generowanie embeddingu dla tekstu: {text[:100]}...")
        response = requests.post(
            'http://localhost:11434/api/embed',
            json={
                'model': 'snowflake-arctic-embed2',
                'input': text
            }
        )
        response.raise_for_status()
        data = response.json()
        
        if st.session_state.get('debug', False):
            st.write("API Response:", data)
            
        if 'embeddings' in data and len(data['embeddings']) > 0:
            # Zapisujemy do cache
            embedding = np.array(data['embeddings'][0])
            st.session_state.embeddings_cache[cache_key] = embedding
            print("Embedding wygenerowany pomy≈õlnie")
            return embedding
        else:
            print(f"B≈ÇƒÖd: Nieoczekiwana struktura odpowiedzi API: {data}")
            return None
    except Exception as e:
        print(f"B≈ÇƒÖd podczas generowania embeddingu: {e}")
        return None

def fetch_sitemap_urls(domain):
    """Fetch and parse URLs from sitemaps, excluding images and handling nested sitemaps."""
    domain = domain.replace("https://", "").replace("http://", "").strip("/")
    sitemap_urls = [
        f"https://{domain}/sitemap.xml",
        f"https://{domain}/sitemap_index.xml",
        f"https://{domain}/console/integration/execute/name/GoogleSitemap",
        f"https://{domain}/robots.txt"
    ]
    all_urls = []

    for sitemap_url in sitemap_urls:
        try:
            response = requests.get(sitemap_url, headers={"User-Agent": "SiteFocusTool/1.0"}, timeout=10)
            response.raise_for_status()
            if "robots.txt" in sitemap_url:
                for line in response.text.splitlines():
                    if line.lower().startswith("sitemap:"):
                        nested_sitemap_url = line.split(":", 1)[1].strip()
                        all_urls.extend(fetch_sitemap_urls_from_xml(nested_sitemap_url, domain, recursive=True))
            else:
                all_urls.extend(fetch_sitemap_urls_from_xml(sitemap_url, domain, recursive=True))
        except requests.RequestException:
            continue
    return list(set(all_urls))

def fetch_sitemap_urls_from_xml(sitemap_url, domain, recursive=False):
    """Fetch URLs from a sitemap XML file."""
    print(f"\n--- Przetwarzanie sitemap: {sitemap_url} ---")
    urls = []
    try:
        response = requests.get(
            sitemap_url, 
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }, 
            timeout=10
        )
        print(f"Status odpowiedzi: {response.status_code}")
        response.raise_for_status()
        
        content_type = response.headers.get('content-type', '')
        print(f"Typ zawarto≈õci: {content_type}")
        
        soup = BeautifulSoup(response.content, "lxml-xml")
        
        # Debugowanie zawarto≈õci XML
        print(f"Zawarto≈õƒá XML (pierwsze 500 znak√≥w):")
        print(response.text[:500])
        
        if soup.find_all("sitemap"):
            print("Znaleziono zagnie≈ºd≈ºone sitemapy")
            for sitemap in soup.find_all("sitemap"):
                loc = sitemap.find("loc")
                if loc:
                    nested_url = loc.text
                    print(f"Zagnie≈ºd≈ºona sitemap: {nested_url}")
                    if recursive:
                        urls.extend(fetch_sitemap_urls_from_xml(nested_url, domain, recursive=True))
        else:
            for loc in soup.find_all("loc"):
                url = loc.text
                if not re.search(r"\.(jpg|jpeg|png|gif|svg|webp|bmp|tif|tiff)$", url, re.IGNORECASE):
                    urls.append(url)
            print(f"Znaleziono {len(urls)} URLi")
            
    except Exception as e:
        print(f"B≈ÇƒÖd podczas przetwarzania {sitemap_url}: {str(e)}")
        print(f"Typ b≈Çƒôdu: {type(e).__name__}")
    
    return urls

def clean_text_from_url(url, domain):
    """Clean URL by removing root domain and extracting readable text."""
    domain = domain.replace("https://", "").replace("http://", "").strip("/")
    url = url.replace(f"https://{domain}/", "").replace(f"http://{domain}/", "")
    text = re.sub(r"[^\w\s]", " ", url)
    text = text.replace("/", " ").replace("_", " ").replace("-", " ")
    return text.strip()

def calculate_site_focus_and_radius(embeddings):
    """Oblicza Site Focus Score i promie≈Ñ."""
    centroid = np.mean(embeddings, axis=0)
    deviations = np.array([
        1 - cosine_similarity(embedding.reshape(1, -1), centroid.reshape(1, -1))[0][0]
        for embedding in embeddings
    ])
    # Teraz deviations to "odleg≈Ço≈õci od centrum"
    return np.mean(deviations), np.std(deviations), centroid, deviations

def plot_gradient_strip_with_indicator(score, title):
    """Visualize the score as a gradient strip with an indicator."""
    plt.figure(figsize=(8, 1))
    gradient = np.linspace(0, 1, 256).reshape(1, -1)
    gradient = np.vstack((gradient, gradient))
    plt.imshow(gradient, aspect="auto", cmap="RdYlGn_r")
    plt.axvline(x=score * 256, color="black", linestyle="--", linewidth=2)
    plt.gca().set_axis_off()
    plt.title(f"{title}: {score * 100:.2f}%")
    st.pyplot(plt)

def plot_3d_tsne(embeddings, urls, centroid, deviations):
    """Interactive 3D t-SNE scatter plot with hover labels."""
    tsne = TSNE(n_components=3, random_state=42, perplexity=min(30, len(embeddings) - 1))
    tsne_results = tsne.fit_transform(np.vstack([embeddings, centroid]))
    centroid_tsne = tsne_results[-1]
    tsne_results = tsne_results[:-1]

    fig = px.scatter_3d(
        x=tsne_results[:, 0],
        y=tsne_results[:, 1],
        z=tsne_results[:, 2],
        color=deviations,
        color_continuous_scale="RdYlGn_r",
        hover_name=urls,
        labels={"color": "Deviation"},
        title="3D t-SNE Projection of Page Embeddings"
    )
    fig.add_scatter3d(
        x=[centroid_tsne[0]],
        y=[centroid_tsne[1]],
        z=[centroid_tsne[2]],
        mode="markers",
        marker=dict(size=15, color="green"),
        name="Centroid"
    )
    st.plotly_chart(fig)

def plot_spherical_distances_optimized(deviations, embeddings, urls):
    """Improved scatter plot showing distances in a spherical layout."""
    num_points = len(deviations)
    angles = np.linspace(0, 2 * np.pi, num_points, endpoint=False)

    fig = px.scatter_polar(
        r=deviations,
        theta=np.degrees(angles),
        color=deviations,
        color_continuous_scale="RdYlGn_r",
        title="Optimized Spherical Plot of Page Distances from Centroid",
        labels={"color": "Deviation"}
    )
    fig.update_traces(
        mode="markers",
        hovertemplate="%{text}<extra></extra>",
        text=urls
    )
    st.plotly_chart(fig)

def analyze_thematic_center(valid_urls, deviations, embeddings):
    """Analyze thematic center of the website."""
    st.header("üéØ Analiza centrum tematycznego")
    
    # 1. Podsumowanie statystyczne
    st.subheader("üìä Statystyki Centrum")
    col1, col2, col3 = st.columns(3)
    
    mean_distance = np.mean([1 - dev for dev in deviations])
    with col1:
        st.metric("≈örednia odleg≈Ço≈õƒá od centrum", f"{mean_distance:.3f}")
    with col2:
        st.metric("Liczba stron", f"{len(valid_urls)}")
    with col3:
        st.metric("Odchylenie standardowe", f"{np.std([1 - dev for dev in deviations]):.3f}")
    
    # 2. Wykres dystrybucji
    st.subheader("üìà Rozk≈Çad odleg≈Ço≈õci od centrum")
    plot_distance_distribution(deviations)
    
    # 3. Lista WSZYSTKICH stron z odleg≈Ço≈õciami
    st.subheader("Lista stron")
    st.markdown("""
    Lista wszystkich stron wraz z ich odleg≈Ço≈õciƒÖ od centrum tematycznego.
    Im wy≈ºszy procent, tym bardziej strona jest reprezentatywna dla g≈Ç√≥wnego tematu.
    """)
    
    # Tworzymy DataFrame ze WSZYSTKIMI stronami
    all_pages_df = pd.DataFrame({
        "URL": valid_urls,
        "Blisko≈õƒá do centrum": [1 - dev for dev in deviations]
    })
    
    # Formatujemy procenty i sortujemy malejƒÖco
    all_pages_df["Blisko≈õƒá do centrum"] = all_pages_df["Blisko≈õƒá do centrum"].apply(lambda x: f"{x*100:.1f}%")
    all_pages_df = all_pages_df.sort_values("Blisko≈õƒá do centrum", ascending=False)
    
    # Dodajemy numeracjƒô
    all_pages_df.index = range(1, len(all_pages_df) + 1)
    all_pages_df.index.name = "Rank"
    
    # Wy≈õwietlamy ca≈Çy DataFrame
    st.dataframe(all_pages_df)
    
    # 4. Wskaz√≥wki interpretacyjne
    st.markdown("""
    ### üí° Jak interpretowaƒá wyniki:
    
    1. **Strony centralne** - Strony z najwy≈ºszym procentem blisko≈õci do centrum najlepiej reprezentujƒÖ g≈Ç√≥wny temat witryny
    2. **Rozk≈Çad odleg≈Ço≈õci** - Wykres pokazuje, jak rozproszone sƒÖ strony wok√≥≈Ç centrum tematycznego:
        - WƒÖski rozk≈Çad ‚Üí Strona jest bardzo sp√≥jna tematycznie
        - Szeroki rozk≈Çad ‚Üí Strona pokrywa r√≥≈ºnorodne tematy
    3. **≈örednia odleg≈Ço≈õƒá** - Im bli≈ºsza 1, tym bardziej sp√≥jna tematycznie jest strona
    """)

# Dodajemy nowƒÖ funkcjƒô do obliczania odleg≈Ço≈õci od URL referencyjnego
def calculate_distances_from_reference(reference_embedding, embeddings, reference_url, valid_urls):
    """Calculate distances from reference URL embedding to all other embeddings."""
    distances = []
    urls = []
    
    # Dodajemy sam URL referencyjny z odleg≈Ço≈õciƒÖ 0
    distances.append(0)
    urls.append(reference_url)
    
    # Obliczamy odleg≈Ço≈õci dla pozosta≈Çych URLi
    for emb, url in zip(embeddings, valid_urls):
        if url != reference_url:  # Pomijamy URL referencyjny w pƒôtli
            similarity = np.dot(reference_embedding, emb)
            distance = 1 - similarity
            distances.append(distance)
            urls.append(url)
            
    return np.array(distances), urls

def collect_closest_pages(domain, urls, distances, n=10):
    """Collect n closest pages from each domain."""
    df = pd.DataFrame({
        "Domain": [domain] * len(urls),
        "URL": urls,
        "Distance": distances
    })
    return df.nlargest(n, "Distance")

def plot_spherical_reference_comparison(reference_url, domains_data):
    """Create spherical plot with reference URL in center and closest pages around."""
    fig = go.Figure()
    
    # Dodajemy punkt centralny (URL referencyjny)
    fig.add_trace(go.Scatter3d(
        x=[0],
        y=[0],
        z=[0],
        mode='markers+text',
        marker=dict(size=10, color='red'),
        text=[reference_url],
        name='Reference URL'
    ))
    
    # Generujemy punkty na sferze dla ka≈ºdej domeny
    colors = px.colors.qualitative.Set3  # R√≥≈ºne kolory dla r√≥≈ºnych domen
    for idx, (domain, df) in enumerate(domains_data.groupby("Domain")):
        # Generujemy punkty na sferze
        n_points = len(df)
        phi = np.linspace(0, 2*np.pi, n_points)
        theta = np.linspace(-np.pi/2, np.pi/2, n_points)
        
        # Konwertujemy odleg≈Ço≈õci na promie≈Ñ (im mniejsza odleg≈Ço≈õƒá, tym bli≈ºej centrum)
        r = 1 + df["Distance"].values
        
        x = r * np.cos(theta) * np.cos(phi)
        y = r * np.cos(theta) * np.sin(phi)
        z = r * np.sin(theta)
        
        fig.add_trace(go.Scatter3d(
            x=x, y=y, z=z,
            mode='markers+text',
            marker=dict(size=6, color=colors[idx % len(colors)]),
            text=df["URL"],
            name=domain,
            hovertemplate="Domain: %{text}<br>Distance: %{customdata}<extra></extra>",
            customdata=df["Distance"]
        ))
    
    fig.update_layout(
        title="Spherical Distribution of Closest Pages to Reference URL",
        scene=dict(
            xaxis_title="X",
            yaxis_title="Y",
            zaxis_title="Z"
        ),
        showlegend=True
    )
    
    st.plotly_chart(fig)

def collect_cross_domain_analysis(reference_embedding, domain_results, n=10):
    """Analyze results across all domains with reference URL as center."""
    all_results = []
    
    for domain_data in domain_results:
        domain = domain_data['domain']
        embeddings = domain_data['embeddings']
        urls = domain_data['urls']
        
        # Obliczamy odleg≈Ço≈õci od URL referencyjnego
        distances = []
        for emb in embeddings:
            similarity = np.dot(reference_embedding, emb)
            distance = 1 - similarity
            distances.append(distance)
        
        # Zbieramy n najbli≈ºszych stron
        closest_indices = np.argsort(distances)[:n]
        for idx in closest_indices:
            all_results.append({
                'Domain': domain,
                'URL': urls[idx],
                'Distance': distances[idx]
            })
    
    return pd.DataFrame(all_results)

def plot_distance_distribution(deviations):
    """Plot distribution of distances from center."""
    fig = go.Figure()
    
    # Konwertujemy deviacje na procenty blisko≈õci do centrum
    proximities = [(1 - dev) * 100 for dev in deviations]
    
    # Dodajemy histogram
    fig.add_trace(go.Histogram(
        x=proximities,
        nbinsx=30,
        name='Rozk≈Çad',
        hovertemplate="Blisko≈õƒá do centrum: %{x:.1f}%<br>Liczba stron: %{y}<extra></extra>"
    ))
    
    # Dodajemy liniƒô ≈õredniej
    mean_proximity = np.mean(proximities)
    fig.add_vline(
        x=mean_proximity,
        line_dash="dash",
        line_color="red",
        annotation_text=f"≈örednia: {mean_proximity:.1f}%",
        annotation_position="top"
    )
    
    # Aktualizujemy layout
    fig.update_layout(
        title="Rozk≈Çad odleg≈Ço≈õci stron od centrum tematycznego",
        xaxis_title="Blisko≈õƒá do centrum (%)",
        yaxis_title="Liczba stron",
        showlegend=False,
        bargap=0.1
    )
    
    # Wy≈õwietlamy wykres
    st.plotly_chart(fig)

def plot_2d_tsne(embeddings, urls, centroid, deviations):
    """2D t-SNE scatter plot with hover labels."""
    # Dodajemy centroid do embeddings
    all_embeddings = np.vstack([embeddings, centroid])
    
    # Wykonujemy t-SNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings) - 1))
    tsne_results = tsne.fit_transform(all_embeddings)
    
    # Oddzielamy wyniki dla stron i centroidu
    pages_tsne = tsne_results[:-1]
    centroid_tsne = tsne_results[-1]
    
    # Tworzymy DataFrame dla plotly
    df = pd.DataFrame({
        'x': pages_tsne[:, 0],
        'y': pages_tsne[:, 1],
        'Distance': deviations,
        'URL': urls
    })
    
    # Tworzymy wykres
    fig = px.scatter(
        df,
        x='x',
        y='y',
        color='Distance',
        color_continuous_scale='RdYlGn_r',
        hover_data=['URL'],
        title='2D t-SNE Projection of Pages',
        labels={'x': 't-SNE 1', 'y': 't-SNE 2'}
    )
    
    # Dodajemy centroid
    fig.add_trace(
        go.Scatter(
            x=[centroid_tsne[0]],
            y=[centroid_tsne[1]],
            mode='markers',
            marker=dict(
                color='red',
                symbol='star',
                size=20
            ),
            name='Centroid',
            hoverinfo='name'
        )
    )
    
    # Aktualizujemy layout
    fig.update_layout(
        width=800,
        height=600,
        showlegend=True
    )
    
    # Wy≈õwietlamy wykres
    st.plotly_chart(fig)

def get_focus_score_interpretation(score):
    """Zwraca interpretacjƒô dla Site Focus Score."""
    if score < 0.30:
        return "üî¥ Niska sp√≥jno≈õƒá tematyczna - strona porusza wiele r√≥≈ºnych temat√≥w"
    elif score < 0.60:
        return "üü° ≈örednia sp√≥jno≈õƒá tematyczna - strona ma kilka g≈Ç√≥wnych obszar√≥w tematycznych"
    else:
        return "üü¢ Wysoka sp√≥jno≈õƒá tematyczna - strona jest mocno skoncentrowana na jednym temacie"

def get_radius_interpretation(radius):
    """Zwraca interpretacjƒô dla Site Radius."""
    if radius < 0.15:
        return "üü¢ Ma≈Çe rozproszenie - tre≈õci sƒÖ bardzo sp√≥jne ze sobƒÖ"
    elif radius < 0.30:
        return "üü° ≈örednie rozproszenie - tre≈õci sƒÖ umiarkowanie zr√≥≈ºnicowane"
    else:
        return "üî¥ Du≈ºe rozproszenie - tre≈õci sƒÖ bardzo zr√≥≈ºnicowane"

# Streamlit Interface
st.title("SiteFocus Tool (Ollama API Version)")


# Debug mode toggle
if 'debug' not in st.session_state:
    st.session_state.debug = False
st.sidebar.checkbox('Debug Mode', value=False, key='debug')

# Test connection to Ollama
try:
    test_embedding = get_embeddings("test")
    if test_embedding is not None:
        st.success("Successfully connected to Ollama")
except Exception as e:
    st.error(f"Could not connect to Ollama. Make sure it's running on localhost:11434\nError: {e}")
    st.stop()

# Najpierw pole URL referencyjnego
reference_url = st.text_input(
    "URL referencyjny (opcjonalnie):", 
    placeholder="https://example.com/page",
    help="Je≈õli nie podasz URL referencyjnego, analiza bƒôdzie przeprowadzona wzglƒôdem centrum tematycznego wszystkich stron."
)

# Nastƒôpnie pole na domeny
domains = st.text_area(
    "Wprowad≈∫ domeny (ka≈ºda w nowej linii):", 
    placeholder="example.com\nexample2.com\nexample3.com",
    help="Wprowadz jednƒÖ lub wiƒôcej domen, ka≈ºdƒÖ w nowej linii"
)

# Inicjalizacja reference_embedding
reference_embedding = None

st.markdown("""
    ‚ÑπÔ∏è **Blisko≈õƒá do centrum** - miara pokazujƒÖca jak blisko centrum tematycznego znajduje siƒô dana strona:
    - Wy≈ºsza warto≈õƒá = strona jest bardziej zgodna z g≈Ç√≥wnƒÖ tematykƒÖ domeny
    - Ni≈ºsza warto≈õƒá = strona bardziej odbiega od typowej tre≈õci na stronie
""")


if st.button("START"):
    if domains:
        # Generujemy embedding dla URL referencyjnego (je≈õli podany)
        if reference_url:
            print(f"Generowanie embeddingu dla URL referencyjnego: {reference_url}")
            reference_text = clean_text_from_url(reference_url, reference_url.split('/')[2])
            reference_embedding = get_embeddings(reference_text)
            if reference_embedding is not None:
                reference_embedding = reference_embedding / norm(reference_embedding)
                st.success("Pomy≈õlnie wygenerowano embedding dla URL referencyjnego")
            else:
                st.error("Nie uda≈Ço siƒô wygenerowaƒá embeddingu dla URL referencyjnego")
        
        # Dzielimy tekst na listƒô domen i usuwamy puste linie
        domain_list = [d.strip() for d in domains.split('\n') if d.strip()]
        
        # Na samym poczƒÖtku
        all_results = []  # Lista na wszystkie wyniki
        
        # G≈Ç√≥wna pƒôtla po domenach
        for domain in domain_list:
            st.subheader(f"Analiza domeny: {domain}")
            print(f"Rozpoczƒôcie przetwarzania domeny: {domain}")
            
            with st.spinner(f"Fetching URLs for {domain}..."):
                urls = fetch_sitemap_urls(domain)
                if not urls:
                    st.error(f"No URLs found for {domain}")
                    continue
                    
                st.info(f"Found {len(urls)} URLs for {domain}")
                
                # Przetwarzanie URLi
                cleaned_texts = [clean_text_from_url(url, domain) for url in urls]
                embeddings = []
                valid_urls = []
                
                for idx, (url, text) in enumerate(zip(urls, cleaned_texts)):
                    embedding = get_embeddings(text)
                    if embedding is not None:
                        normalized_embedding = embedding / norm(embedding)
                        embeddings.append(normalized_embedding)
                        valid_urls.append(url)
                
                # Analiza pojedynczej domeny
                if len(embeddings) > 0:
                    embeddings = np.array(embeddings)
                    site_focus_score, site_radius, centroid, deviations = calculate_site_focus_and_radius(embeddings)
                    
                    # 1. Metryki z interpretacjƒÖ
                    st.subheader("üìä Metryki sp√≥jno≈õci tematycznej")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric(label="Site Focus Score", value=f"{site_focus_score:.2%}")
                        st.markdown(get_focus_score_interpretation(site_focus_score))
                        st.markdown("""
                        **Site Focus Score (Sp√≥jno≈õƒá tematyczna)**:

                        - **<30%** - Niska sp√≥jno≈õƒá tematyczna  
                        - Strona porusza wiele r√≥≈ºnych, niepowiƒÖzanych temat√≥w  
                        - Tre≈õci sƒÖ bardzo zr√≥≈ºnicowane  
                        - Typowe dla portali og√≥lnotematycznych lub agregator√≥w tre≈õci  

                        - **30-60%** - ≈örednia sp√≥jno≈õƒá tematyczna  
                        - Strona ma kilka g≈Ç√≥wnych obszar√≥w tematycznych  
                        - Tre≈õci sƒÖ powiƒÖzane, ale zr√≥≈ºnicowane  
                        - Typowe dla portali bran≈ºowych lub blog√≥w o szerokiej tematyce  

                        - **>60%** - Wysoka sp√≥jno≈õƒá tematyczna  
                        - Strona koncentruje siƒô na jednym g≈Ç√≥wnym temacie  
                        - Tre≈õci sƒÖ ≈õci≈õle ze sobƒÖ powiƒÖzane  
                        - Typowe dla specjalistycznych stron i blog√≥w tematycznych  
                        """)
                        
                    with col2:
                        st.metric(label="Site Radius", value=f"{site_radius:.2%}")
                        st.markdown(get_radius_interpretation(site_radius))
                        st.markdown("""
                                    **Site Radius (Rozproszenie tre≈õci)**:

                                    - **<15%** - Ma≈Çe rozproszenie  
                                    - Tre≈õci sƒÖ bardzo sp√≥jne ze sobƒÖ  
                                    - Poszczeg√≥lne strony trzymajƒÖ siƒô g≈Ç√≥wnego tematu  
                                    - Wskazuje na konsekwentnƒÖ strategiƒô tre≈õci  

                                    - **15-30%** - ≈örednie rozproszenie  
                                    - Tre≈õci sƒÖ umiarkowanie zr√≥≈ºnicowane  
                                    - WystƒôpujƒÖ odstƒôpstwa od g≈Ç√≥wnego tematu  
                                    - Typowe dla stron z r√≥≈ºnorodnymi podsekcjami  

                                    - **>30%** - Du≈ºe rozproszenie  
                                    - Tre≈õci znaczƒÖco r√≥≈ºniƒÖ siƒô od siebie  
                                    - Du≈ºe odchylenia od g≈Ç√≥wnego tematu  
                                    - Mo≈ºe wskazywaƒá na brak sp√≥jnej strategii tre≈õci  
                                    """)
                        

                    
                    # Dodajemy szczeg√≥≈Çowe wyja≈õnienie skali
                    
                    

                    st.markdown("""üí° **Optymalne warto≈õci** zale≈ºƒÖ od typu strony i jej przeznaczenia. Dla wyspecjalizowanego bloga tematycznego korzystne bƒôdƒÖ wysokie warto≈õci Site Focus Score i niskie Site Radius. Dla portalu informacyjnego naturalne bƒôdƒÖ ≈õrednie warto≈õci obu metryk.
                    """)
                    
                    # 2. Szczeg√≥≈Çowe wizualizacje z opisami
                    st.markdown("---")
                    st.subheader("siteFocusScore")
                    st.markdown(""" 
                    **Site Focus Score** odzwierciedla, jak mocno tre≈õci na stronie sƒÖ skupione wok√≥≈Ç jednego obszaru tematycznego.  
                    Wy≈ºszy wynik oznacza wiƒôkszƒÖ sp√≥jno≈õƒá tematycznƒÖ.
                    """)
                    
                    st.markdown(f"""
                        <div style='margin: 20px 0;'>
                            <p style='text-align: center; font-size: 1.2em;'>siteFocusScore: {site_focus_score:.2%}</p>
                            <div style='background: linear-gradient(to right, #00ff00, #ffff00, #ff0000); height: 30px; position: relative; border-radius: 4px;'>
                                <div style='position: absolute; left: {site_focus_score*100}%; border-left: 2px dashed black; height: 100%;'></div>
                            </div>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown("---")
                    st.subheader("siteRadius")
                    st.markdown("""
                    **Site Radius** mierzy, jak bardzo poszczeg√≥lne strony odbiegajƒÖ od g≈Ç√≥wnego tematu strony.  
                    Mniejszy promie≈Ñ oznacza wiƒôkszƒÖ sp√≥jno≈õƒá tre≈õci.
                    """)
                    
                    st.markdown(f"""
                        <div style='margin: 20px 0;'>
                            <p style='text-align: center; font-size: 1.2em;'>siteRadius: {site_radius:.2%}</p>
                            <div style='background: linear-gradient(to right, #00ff00, #ffff00, #ff0000); height: 30px; position: relative; border-radius: 4px;'>
                                <div style='position: absolute; left: {site_radius*100}%; border-left: 2px dashed black; height: 100%;'></div>
                            </div>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    # Dodajemy 2D t-SNE visualization
                    st.subheader("2D t-SNE Projection")
                    plot_2d_tsne(embeddings, valid_urls, centroid, deviations)
                    
                    # Je≈õli mamy URL referencyjny, dodajemy podsumowanie najbli≈ºszych stron
                    if reference_embedding is not None:
                        st.subheader("üéØ Top 10 stron najbli≈ºszych URL referencyjnemu")
                        
                        # Obliczamy odleg≈Ço≈õci od URL referencyjnego
                        distances_from_ref = []
                        for emb in embeddings:
                            similarity = np.dot(reference_embedding, emb)
                            distance = 1 - similarity
                            distances_from_ref.append(distance)
                        
                        # Tworzymy DataFrame z wynikami
                        domain_results_df = pd.DataFrame({
                            'URL': valid_urls,
                            'Distance': distances_from_ref
                        })
                        
                        # Sortujemy i bierzemy top 10
                        domain_top_10 = domain_results_df.nsmallest(10, 'Distance')
                        
                        # Dodajemy numeracjƒô
                        domain_top_10.index = range(1, len(domain_top_10) + 1)
                        domain_top_10.index.name = "Rank"
                        
                        # Formatujemy odleg≈Ço≈õci na procenty podobie≈Ñstwa
                        domain_top_10['Similarity'] = domain_top_10['Distance'].apply(lambda x: f"{(1-x)*100:.1f}%")
                        
                        # Wy≈õwietlamy tabelƒô
                        st.dataframe(domain_top_10[['URL', 'Similarity']])
                        
                        # Dodajemy kr√≥tkie podsumowanie
                        mean_similarity = (1 - np.mean(distances_from_ref)) * 100
                        st.markdown(f"""
                        **Podsumowanie dla domeny {domain}:**
                        - ≈örednie podobie≈Ñstwo do URL referencyjnego: {mean_similarity:.1f}%
                        - Liczba przeanalizowanych stron: {len(valid_urls)}
                        """)
                    
                    # Analiza centrum tematycznego
                    analyze_thematic_center(valid_urls, deviations, embeddings)
                    
                    # Wizualizacje 3D
                    st.header("üåê Wizualizacje 3D")
                    st.subheader("3D t-SNE Projection")
                    plot_3d_tsne(embeddings, valid_urls, centroid, deviations)
                    
                    st.subheader("Spherical Distance Plot")
                    plot_spherical_distances_optimized(deviations, embeddings, valid_urls)
                    
                    # Zbieranie danych do analizy cross-domain
                    if reference_embedding is not None:
                        for url, emb in zip(valid_urls, embeddings):
                            similarity = np.dot(reference_embedding, emb)
                            distance = 1 - similarity
                            all_results.append({
                                'Domain': domain,
                                'URL': url,
                                'Distance': distance,
                                'Embedding': emb
                            })

        # CA≈ÅKOWICIE POZA PƒòTLƒÑ - analiza cross-domain
        if reference_url and len(all_results) > 0:
            st.header("üåç Por√≥wnanie domen", anchor="porownanie-domen")
            st.subheader("Najbli≈ºsze strony wzglƒôdem URL referencyjnego")
            
            # Tworzymy DataFrame ze wszystkich wynik√≥w
            cross_domain_results = pd.DataFrame(all_results)
            
            # Przekszta≈Çcamy reference_embeddings na w≈Ça≈õciwy kszta≈Çt
            if len(reference_embedding.shape) == 1:
                reference_embedding = reference_embedding.reshape(1, -1)
            reference_centroid = reference_embedding
            
            # Przeliczamy odleg≈Ço≈õci dla wszystkich URLi
            url_distances = {}
            for _, row in cross_domain_results.iterrows():
                url = row['URL']
                embeddings = np.array(row['Embedding'])
                if len(embeddings.shape) == 1:
                    embeddings = embeddings.reshape(1, -1)
                distance = 1 - cosine_similarity(reference_centroid, embeddings)[0][0]
                url_distances[url] = distance
            
            # Aktualizujemy DataFrame o nowe odleg≈Ço≈õci
            cross_domain_results['New_Distance'] = cross_domain_results['URL'].map(url_distances)
            
            # Sortujemy po nowych odleg≈Ço≈õciach
            results_table = (cross_domain_results
                .sort_values('New_Distance')
                .groupby('Domain')
                .head(10)
                .reset_index(drop=True))
            
            # Tworzymy wiersz dla URL referencyjnego
            reference_row = pd.DataFrame([{
                'Domain': urlparse(reference_url).netloc,
                'URL': reference_url,
                'New_Distance': 0.0
            }])
            
            # ≈ÅƒÖczymy URL referencyjny z wynikami
            results_table = pd.concat([reference_row, results_table], ignore_index=True)
            
            # Wy≈õwietlamy tabelƒô z nowymi odleg≈Ço≈õciami
            st.dataframe(
                results_table[['Domain', 'URL', 'New_Distance']],
                column_config={
                    "Domain": "Domena",
                    "URL": "Adres URL",
                    "New_Distance": st.column_config.NumberColumn(
                        "Odleg≈Ço≈õƒá od centroidu",
                        format="%.3f",
                    )
                },
                hide_index=True
            )
            
            # Dodajemy wykres polarny u≈ºywajƒÖc zoptymalizowanej funkcji
            st.markdown("---")
            st.subheader("üéØ Optimized Spherical Plot of Page Distances from Centroid")
            
            plot_spherical_distances_optimized(
                deviations=results_table['New_Distance'].values,
                embeddings=None,  # Nie potrzebujemy embedding√≥w do tego wykresu
                urls=results_table['URL'].values
            )

            st.markdown("""
            ### üéØ Interpretacja wykresu polarnego:
            - ≈örodek wykresu reprezentuje URL referencyjny (odleg≈Ço≈õƒá = 0)
            - Odleg≈Ço≈õƒá od ≈õrodka pokazuje r√≥≈ºnicƒô tematycznƒÖ:
                * Bli≈ºej ≈õrodka = tre≈õƒá bardziej podobna do referencyjnej
                * Dalej od ≈õrodka = wiƒôksza r√≥≈ºnica w tre≈õci
            - Kolor punkt√≥w reprezentuje odleg≈Ço≈õƒá (zielony = blisko, czerwony = daleko)
            """)

